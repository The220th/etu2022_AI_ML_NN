{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9W8DPpAmL2nD"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "hw4JJtGCbGNA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# dataset preparation\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# NN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models as torch_models\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Other\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import hashlib\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(5051)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "num_embeddings = 10000 # look num_words in file \"prepare_ds.py\"\n",
    "                             # or it must be len(SrachSet.mydict.keys()) == 229520 ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc5Oun-eMXFd"
   },
   "source": [
    "# Подготовка датасета\n",
    "\n",
    "Надо что-нибудь написать... Потом... Как-нибудь..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9RR1EAUXMyu7"
   },
   "outputs": [],
   "source": [
    "# Делаем свой кастомный датасет\n",
    "\n",
    "\n",
    "class SrachSet(torch.utils.data.Dataset):\n",
    "    mydict = None\n",
    "    \n",
    "    def __init__(self, _type: str, _path: str = \"out.csv\", _json_path: str = \"word_index_dict.json\"):\n",
    "        self.__type = _type\n",
    "        assert self.__type in [\"train\", \"valid\", \"test\"]\n",
    "        self.mypath = _path\n",
    "        self.mypath_dict = _json_path\n",
    "        self.laeblo = []\n",
    "        self.myds = []\n",
    "        \n",
    "        df = pd.read_csv(self.mypath)\n",
    "        df = df.reset_index()\n",
    "        for i, row_i in df.iterrows():\n",
    "            self.laeblo.append(   [row_i[\"toxics\"], row_i[\"obscene\"], row_i[\"threat\"], row_i[\"insult\"], row_i[\"identity_hate\"]]   )\n",
    "            self.myds.append  (   list(map(int, str(row_i[\"comment\"]).split()))   )\n",
    "        \n",
    "        if(self.__type == \"train\"):\n",
    "            self.myds = self.__split_ds(self.myds)[0]\n",
    "            self.laeblo = self.__split_ds(self.laeblo)[0]\n",
    "        elif(self.__type == \"valid\"):\n",
    "            self.myds = self.__split_ds(self.myds)[1]\n",
    "            self.laeblo = self.__split_ds(self.laeblo)[1]\n",
    "        elif(self.__type == \"test\"):\n",
    "            self.myds = self.__split_ds(self.myds)[2]\n",
    "            self.laeblo = self.__split_ds(self.laeblo)[2]\n",
    "        else:\n",
    "            raise Exception(\"wtf\")\n",
    "        random.shuffle(self.myds)\n",
    "\n",
    "        if(SrachSet.mydict == None):\n",
    "            S = None\n",
    "            with open(self.mypath_dict, 'r', encoding=\"utf-8\") as temp:\n",
    "                S = temp.read()\n",
    "            buff = json.loads(S)\n",
    "            SrachSet.mydict = {v: k for k, v in buff.items()}\n",
    "        \n",
    "    def __split_ds(self, a: list) -> \"(list, list, list)\":\n",
    "        split_num = int(0.7*len(a) +0.5)\n",
    "        train = a[:split_num]               # 70 %\n",
    "        test_and_valid  = a[split_num:]     # 30 %\n",
    "        \n",
    "        split_num = int((1/3)*len(test_and_valid) +0.5)\n",
    "        valid = test_and_valid[:split_num]  # 10% = 1/3 * 30%\n",
    "        test = test_and_valid[split_num:]   # 20% = 2/3 * 30%\n",
    "        return (train, valid, test)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.myds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur = self.myds[idx]\n",
    "        wa  = self.laeblo[idx]\n",
    "\n",
    "        #tenz_cur = torch.LongTensor(cur)\n",
    "        #cur = torch.Tensor(cur)\n",
    "        cur = torch.IntTensor(cur)\n",
    "        wa = torch.IntTensor(wa)\n",
    "        \n",
    "        return cur, wa\n",
    "\n",
    "    def convert_tensor_to_label(tensor: \"Tensor\"):\n",
    "        res = [\"toxics: \", \"obscene: \", \"threat: \", \"insult: \", \"identity_hate: \"]\n",
    "        li = 0\n",
    "        for el_i in tensor:\n",
    "            if el_i != 0:\n",
    "                res[li] = res[li] + \"yes\"\n",
    "            else:\n",
    "                res[li] = res[li] + \"no\"\n",
    "            li-=-1\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def show_seq(seq_tensor: \"Tensor\") -> str:\n",
    "        S = \"\"\n",
    "        for item_i in seq_tensor:\n",
    "            kye = int(item_i)\n",
    "            if(kye != 0):\n",
    "                S += f\"{SrachSet.mydict[kye]} \"\n",
    "        return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция инициализации датасетлоудеров:\n",
    "trainloader, validloader, testloader = None, None, None\n",
    "def dsloaders_init(batch_size):\n",
    "    global trainloader\n",
    "    global validloader\n",
    "    global testloader\n",
    "    mytrainset = SrachSet(\"train\")\n",
    "    trainloader = torch.utils.data.DataLoader(mytrainset, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "    myvalidset = SrachSet(\"valid\")\n",
    "    validloader = torch.utils.data.DataLoader(myvalidset, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "    mytestset = SrachSet(\"test\")\n",
    "    testloader = torch.utils.data.DataLoader(mytestset, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция показа содержимого\n",
    "def show_srach(srach_rensor: \"Tensor\", label_tensor: \"Tensor\") -> None:\n",
    "    srach_list = [int(i) for i in srach_rensor]\n",
    "    print(srach_list)\n",
    "    print(\"<--->\")\n",
    "    print(SrachSet.show_seq(srach_rensor))\n",
    "    print(f\"It is: {SrachSet.convert_tensor_to_label(label_tensor)}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3qnvV5mxOLxh"
   },
   "outputs": [],
   "source": [
    "dsloaders_init(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "XQgZWAVyORKO",
    "outputId": "0d8f6c94-f0d3-411d-ba29-f913524ec94a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170, 380, 1, 258, 8, 5, 1764, 17, 1, 743, 218, 312, 26, 6, 60, 267, 77, 9, 10, 517, 2669, 12, 290, 117, 93]\n",
      "<--->\n",
      "hi clearly the subject is a model as the external links show but i don't understand why that in itself consideration for speedy deletion thanks \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2562]\n",
      "<--->\n",
      "and liar \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[146, 2, 120, 619, 50, 114, 15, 90, 46, 122, 36, 250, 17, 50, 18, 64, 2, 2028, 27, 372, 4, 14, 12, 25, 4489]\n",
      "<--->\n",
      "right to say whatever they want on their talk pages so long as they are here to build an encyclopedia and not for or motives \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[369, 9, 3725, 8, 1, 1043, 3, 843, 49, 51, 40, 18, 843, 62, 34, 14, 982, 55, 3, 1, 1385, 441, 10, 1, 956]\n",
      "<--->\n",
      "saying that judaism is the religion of jews just like there are jews who do not speak any of the languages listed in the infobox \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[18, 50, 89, 419, 284, 73, 7, 2752, 95, 160, 11, 129, 88, 7, 7, 121, 2, 16, 803, 7, 18, 14, 38, 29, 320]\n",
      "<--->\n",
      "are they now start making up you didnt even read it again did you you need to be banned you are not what wikipedia needs \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[8, 128, 944, 1, 681, 8, 773, 281, 975, 20, 11, 3634, 20, 11, 711, 20, 11, 1337, 41, 16, 1756, 458, 556, 8, 1792]\n",
      "<--->\n",
      "is really lol the truth is obvious s eat with it sleep with it live with it thats all be honest ok god is watching \n",
      "It is: ['toxics: yes', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[1599, 399, 10, 1, 604, 39, 7, 48, 805, 20, 1, 108, 62, 1887, 296, 1, 1295, 1475, 148, 50, 18, 1709, 15, 1, 5089]\n",
      "<--->\n",
      "numerous times in the past can you please deal with the user who keeps adding the episode titles before they are confirmed on the schedule \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[14, 65, 9, 11, 56, 16, 1367, 12, 290, 117, 17, 1, 482, 8, 100, 15, 1857, 4, 8, 125, 325, 592, 2, 105, 946]\n",
      "<--->\n",
      "not think that it should be nominated for speedy deletion as the project is well on track and is much notable due to its size \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[220, 40, 5, 1580, 3, 325, 74, 4, 6, 45, 119, 7, 20, 9, 85, 9687, 60, 16, 5284, 230, 243, 73, 1, 97, 811]\n",
      "<--->\n",
      "person there a lots of notable people and i will help you with that time permitting don't be discouraged though keep up the good job \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[1, 1035, 932, 3, 831, 702, 1, 112, 29, 23, 74, 63, 1, 193, 42, 5, 600, 10, 2654, 2206, 73, 32, 4, 5111, 2530]\n",
      "<--->\n",
      "the poor quality of what's generally the first wikipedia article people see the us has a similar in chicago built up by and kelly martin \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 1014, 1002, 213, 6, 60, 68, 22, 71, 990, 2, 120, 26, 1511, 5, 166, 353, 13, 383, 151, 602, 46, 1741, 2921, 365]\n",
      "<--->\n",
      "april 2011 utc i don't know if i'm supposed to say but here's a link delete this message after reading talk 06 56 4 \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 88, 7, 160, 21, 349, 51, 7, 24, 4, 13, 8, 62, 54, 19, 126, 4533, 127, 84, 7, 886]\n",
      "<--->\n",
      "did you read your post like you was and this is who we have editing omg where were you born \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[256, 215, 37, 15, 22, 30, 46, 28, 30, 46, 28, 25, 2815, 21, 15, 13, 28, 4, 83, 201, 148, 1, 211, 129, 209]\n",
      "<--->\n",
      "questions ask me on if my talk page my talk page or lick your on this page and then place before the question again off \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[352, 675, 933, 31, 5, 1503, 25, 371, 933, 32, 739, 77, 18, 40, 855, 3561, 17, 22, 256, 77, 101, 13, 356, 1074, 413]\n",
      "<--->\n",
      "re posted directly from a magazine or written directly by themselves why are there statements ending as if questions why does this seem entirely pov \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[272, 961, 38, 427, 961, 102, 13, 2557, 61, 92, 167, 25, 7564, 65, 35, 11, 89, 163, 2, 21, 577, 3778, 154, 284, 2413]\n",
      "<--->\n",
      "word purpose what possible purpose could this serve other than own or curiosity think about it now back to your small minded list making task \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1106, 206, 11, 15, 29, 1258, 1039, 4, 7, 86, 94, 44, 567]\n",
      "<--->\n",
      "definitely put it on wikipedia requested pictures and you may get one 7 \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# получаем несколько случайных обучающих $%#5^\n",
    "dataiter = iter(trainloader)\n",
    "seqs_tenz, labels_tenz = next(dataiter)\n",
    "\n",
    "for seq_tenz_i, label_tenz_i in zip(seqs_tenz, labels_tenz):\n",
    "    show_srach(seq_tenz_i, label_tenz_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "RQbQYemeORgO",
    "outputId": "8715dd5c-34d0-4ba9-9681-06eb80db540a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400, 111, 17, 2778, 4, 1209, 1211, 234, 9, 71, 14, 1074, 186, 38, 172, 1789, 33, 26, 6, 43, 454, 7, 5792, 11, 209]\n",
      "<--->\n",
      "users such as sockpuppets and spam accounts i'll that i'm not entirely sure what you're playing at but i would suggest you knock it off \n",
      "It is: ['toxics: yes', 'obscene: yes', 'threat: no', 'insult: yes', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 65, 6, 86, 19, 4628, 59, 3, 1, 665, 20, 1, 6346, 3996]\n",
      "<--->\n",
      "i think i may have solved some of the problems with the ethical slut \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[4, 371, 76, 10, 1710, 2529, 2320, 20, 1, 760, 10, 255, 69, 371, 10, 255, 26, 129, 13, 133, 8, 286, 191, 33, 4678]\n",
      "<--->\n",
      "and written out in roman letters whereas with the series in english it's written in english but again this discussion is probably better at wt \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[19, 2161, 726, 15, 6773, 5132, 1924, 249, 2789, 306, 930, 3, 53, 8, 1692, 15, 1, 421, 28, 5274, 992, 200, 173, 32, 252]\n",
      "<--->\n",
      "have extreme views on immigration barack obama gay marriage etc none of which is addressed on the main page — preceding unsigned comment added by • \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[56, 677, 5, 5536, 125, 445, 5, 1072, 40, 8, 5, 686, 9, 2474, 6, 67, 4319, 2, 254, 1, 26, 52, 24, 1199, 494]\n",
      "<--->\n",
      "should kill a noble much less a king there is a story that richard i his killer to remove the but he was killed anyway \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[34, 5, 343, 783, 64, 22, 1172, 51, 2, 1212, 20, 126, 82, 1, 641, 2, 94, 501, 6, 226, 7, 39, 119, 193, 76]\n",
      "<--->\n",
      "do a bit seriously here if you'd like to experiment with editing use the sandbox to get started i hope you can help us out \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 608, 398, 9230, 806, 8119, 1067, 8707, 1067, 7928, 1067, 8120, 806, 1067, 1081, 1174]\n",
      "<--->\n",
      "class c b1 n b2 y b3 y b4 y b5 n y importance low \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[76, 32, 7631, 27, 1169, 26, 13, 8, 269, 10, 1217, 6, 102, 354, 511, 26, 226, 11, 279, 792, 2, 123, 134, 111, 1148]\n",
      "<--->\n",
      "out by filing an rfc but this is different in ways i could perhaps explain but hope it isn't necessary to go into such detail \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 316, 63, 30, 736, 10, 35, 6146]\n",
      "<--->\n",
      "about consensus see my reply in about concensus \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 175, 5016, 5, 827, 7212, 3, 911, 4, 538, 1155, 873, 7979, 7, 3571]\n",
      "<--->\n",
      "stop censoring a complete accounting of actions and during november 22 1963 you bastards \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 4033, 302, 4, 116, 1262, 1503, 26, 6, 19, 14, 446, 157, 10, 1, 3867, 399]\n",
      "<--->\n",
      "the guardian yes and new york magazine but i have not seen him in the ny times \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[37, 2453, 6, 24, 75, 1789, 319, 89, 6, 79, 14, 165, 2, 353, 30, 150, 15, 35, 1, 3922, 15, 1, 155, 591, 28]\n",
      "<--->\n",
      "me yesterday i was only playing around now i am not going to delete my add on about the gang on the 2 game page \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1467, 1368, 21, 1382, 45, 144, 346, 3603, 998, 87, 2, 90, 8292]\n",
      "<--->\n",
      "3rr hopefully your warnings will work maybe it'll bring them to their senses \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[655, 15, 701, 692, 54, 99, 4, 86, 152, 19, 81, 15, 1, 469, 711, 2553, 53, 19, 2970, 173, 1691, 12, 367, 342, 135]\n",
      "<--->\n",
      "terms on shows indeed we had and may still have articles on the related live incidents which have occurred added ref for correct term used \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[131, 2682, 230, 6, 49, 226, 9, 41, 13, 39, 16, 2212, 174, 381, 4, 20, 4336, 1104, 208, 2, 1, 5124, 4, 1, 8050]\n",
      "<--->\n",
      "most intelligent though i just hope that all this can be resolved without further and with solutions acceptable both to the palestinians and the israelis \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 21, 9523, 845, 320, 5, 97]\n",
      "<--->\n",
      "and your supremacist head needs a good \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# получаем несколько случайных валидационных ?!№#@\n",
    "dataiter = iter(validloader)\n",
    "seqs_tenz, labels_tenz = next(dataiter)\n",
    "\n",
    "for seq_tenz_i, label_tenz_i in zip(seqs_tenz, labels_tenz):\n",
    "    show_srach(seq_tenz_i, label_tenz_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "kjSiuFP4OSBP",
    "outputId": "31b743a6-9227-491c-b239-6ab6d855c547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[819, 2, 617, 6, 65, 50, 18, 103, 97, 1020, 3, 2484, 192, 6, 79, 5993, 919, 17, 6, 3147, 155, 3, 1, 231, 4]\n",
      "<--->\n",
      "allowed to stay i think they are very good examples of tom however i am admittedly bias as i invented 2 of the 3 and \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[103, 259, 2534, 5138, 10, 393, 3377, 5, 951, 386, 48, 63, 29, 635, 2534, 28, 171, 1221, 934, 12, 422, 137, 7, 12, 839]\n",
      "<--->\n",
      "very few aircraft appearances in media warrant a special mention please see wikipedia wikiproject aircraft page content popular culture for guidelines thank you for contributing \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[630, 8, 52, 8, 2911, 531, 4859, 4, 828, 5331, 2029, 169, 461, 29, 397, 898, 1235, 1307, 322, 6, 16, 3060, 854, 1474, 2420]\n",
      "<--->\n",
      "guy is he is deliberately removing punctuation and creating grammatical mistakes http en wikipedia org w index php title i be alive action diff oldid \n",
      "It is: ['toxics: yes', 'obscene: yes', 'threat: no', 'insult: yes', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[698, 35, 193, 436, 1, 801, 109, 38, 8, 1, 109, 9, 7, 43, 2, 1811, 7557, 5349, 2901, 8385, 7557, 5349, 2901, 2050, 1734]\n",
      "<--->\n",
      "explanation about us regarding the article's name what is the name that you would to propose malaysia airlines flight speculations malaysia airlines flight fringe theories \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[2492, 2, 644, 5, 358, 145, 33, 72, 2103, 52, 2507, 10, 4004, 72, 250, 45, 52, 16, 553, 2, 243, 73, 20, 13, 7565]\n",
      "<--->\n",
      "forced to become a bitch look at how fake he acts in interviews how long will he be able to keep up with this miserable \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 736, 2, 354, 6, 56, 16, 7, 9787]\n",
      "<--->\n",
      "reply to perhaps i should be you dummy \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[17, 9376, 500, 618, 138, 571, 2666, 6241, 30, 28, 2375, 5702, 4, 175, 5926, 21, 3895, 8393, 72, 101, 13, 197, 119, 262, 46]\n",
      "<--->\n",
      "as boo f g i've published papers where's my page quit crying and stop hating your superior peers how does this actually help anyone talk \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[121, 5, 951, 113, 12, 428, 3, 130, 61, 1064, 111, 17, 3743, 1808, 614, 1201, 2642, 5891, 3737, 1064, 4, 62, 1089, 38, 274]\n",
      "<--->\n",
      "need a special section for each of many other groups such as blacks muslims political religious horse lovers foot groups and who knows what others \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[148, 531, 1, 451, 31, 1, 28, 115, 2811, 4, 227, 18, 649, 2, 2503, 543, 316, 35, 1, 699, 3, 5, 28, 137, 7]\n",
      "<--->\n",
      "before removing the notice from the page these notices and comments are needed to establish community consensus about the status of a page thank you \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<--->\n",
      "\n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4399, 4755, 434, 1470, 26, 587, 5, 6922, 5, 6922, 587, 5]\n",
      "<--->\n",
      "ray lewis american football but he's a murderer a murderer he's a \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[2083, 192, 1, 139, 8, 384, 12, 4243, 4, 61, 7607, 1, 888, 1434, 3, 1, 2236, 8, 468, 4, 2555, 13, 78, 9, 405]\n",
      "<--->\n",
      "reporting however the same is true for cnn and other networks the primary focus of the channel is news and imo this edit that idea \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7242, 49, 2033, 379]\n",
      "<--->\n",
      "yup just plain stupid \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[60, 63, 13, 6401, 503, 587, 5, 4, 1199, 67, 2305, 190, 11, 4, 52, 293, 67, 967, 10, 1, 2666, 36, 38, 634, 634]\n",
      "<--->\n",
      "don't see this guy's notability he's a and killed his parents that's it and he got his face in the papers so what     \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[10, 1395, 1975, 8, 331, 5, 25, 25, 1, 3, 5, 31, 1, 3490, 118, 27, 4254, 8463, 1773, 32, 559, 9625, 691, 922, 922]\n",
      "<--->\n",
      "in late latin is called a or or the of a from the shape source an anglo saxon dictionary by t 1921 cited html html \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n",
      "[9, 243, 1, 2117, 3, 38, 8, 590, 27, 387, 1244, 10, 1, 1579, 927, 32, 90, 167, 272, 50, 433, 11, 933, 538, 1453]\n",
      "<--->\n",
      "that keep the balance of what is obviously an important controversy in the catholic church by their own word they mentioned it directly during mass \n",
      "It is: ['toxics: no', 'obscene: no', 'threat: no', 'insult: no', 'identity_hate: no']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# получаем несколько случайных тестовых %1№2!#?\n",
    "dataiter = iter(testloader)\n",
    "seqs_tenz, labels_tenz = next(dataiter)\n",
    "\n",
    "for seq_tenz_i, label_tenz_i in zip(seqs_tenz, labels_tenz):\n",
    "    show_srach(seq_tenz_i, label_tenz_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkZmXTYIVeZG",
    "outputId": "e941ba29-e44f-4269-ac75-3ea902199c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA? True\n",
      "Devices count: 1\n",
      "Current device: 0\n",
      "Device: <torch.cuda.device object at 0x7faf7293bf40>\n",
      "GPU name: NVIDIA GeForce GTX 1070 Ti\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Смотрим, что вообще есть видеокарта\n",
    "\n",
    "print(f\"CUDA? {torch.cuda.is_available()}\")\n",
    "print(f\"Devices count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device: {torch.cuda.device(0)}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFJj3tRzQY8B"
   },
   "source": [
    "# Делаем собственную архитектуру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучающий класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модифицируем (видимо, не вперый раз) вспомогательный класс под текущую задачу\n",
    "class Optimization:\n",
    "    \"\"\"Optimization is a helper class that allows training, validation, prediction.\n",
    "\n",
    "    Optimization is a helper class that takes model, loss function, optimizer function\n",
    "    learning scheduler (optional), early stopping (optional) as inputs. In return, it\n",
    "    provides a framework to train and validate the models, and to predict future values\n",
    "    based on the models.\n",
    "\n",
    "    Attributes:\n",
    "        model (nn.Module): Model class\n",
    "        loss_fn (torch.nn.modules.Loss): Loss function to calculate the losses\n",
    "        optimizer (torch.optim.Optimizer): Optimizer function to optimize the loss function\n",
    "        train_losses (list[float]): The loss values from the training\n",
    "        val_losses (list[float]): The loss values from the validation\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): Model class\n",
    "            loss_fn (torch.nn.modules.Loss): Loss function to calculate the losses\n",
    "            optimizer (torch.optim.Optimizer): Optimizer function to optimize the loss function\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.err_rates = []\n",
    "        \n",
    "    def train_step(self, x, y) -> \"loss\":\n",
    "        \"\"\"The method train_step completes one step of training.\n",
    "\n",
    "        Given the features (x) and the target values (y) tensors, the method completes\n",
    "        one step of the training. First, it activates the train mode to enable back prop.\n",
    "        After generating predicted values (yhat) by doing forward propagation, it calculates\n",
    "        the losses by using the loss function. Then, it computes the gradients by doing\n",
    "        back propagation and updates the weights by calling step() function.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor for features to train one step\n",
    "            y (torch.Tensor): Tensor for target values to calculate losses\n",
    "\n",
    "        \"\"\"\n",
    "        # Sets model to train mode\n",
    "        self.model.train() # dropout is here, so call it\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(yhat, y)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, n_epochs, test_loader = None) -> None:\n",
    "        \"\"\"The method train performs the model training\n",
    "\n",
    "        The method takes DataLoaders for training and validation datasets, batch size for\n",
    "        mini-batch training, number of epochs to train.\n",
    "        Then, it carries out the training by iteratively calling the method train_step for\n",
    "        n_epochs times. If early stopping is enabled, then it  checks the stopping condition\n",
    "        to decide whether the training needs to halt before n_epochs steps. Finally, it saves\n",
    "        the model in a designated file path.\n",
    "\n",
    "        Args:\n",
    "            train_loader (torch.utils.data.DataLoader): DataLoader that stores training data\n",
    "            val_loader (torch.utils.data.DataLoader): DataLoader that stores validation data\n",
    "            n_epochs (int): Number of epochs, i.e., train steps, to train\n",
    "            test_loader (torch.utils.data.DataLoader): set it if need error_rate after one epoch\n",
    "        \"\"\"\n",
    "        model_path = f'model_{datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}'\n",
    "\n",
    "        start_time_gl = time.time()\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            start_time_ep = time.time()\n",
    "            \n",
    "            gi, N = 0, len(train_loader)\n",
    "            batch_losses = []\n",
    "            print(f\"({epoch}/{n_epochs}) {N}: \", end=\"\")\n",
    "            start_time_bt = time.time()\n",
    "            for x_batch, y_batch in train_loader:\n",
    "\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                batch_losses.append(loss)\n",
    "                gi+=1\n",
    "                print(f\"_{gi}\", end=\"\")\n",
    "            print(f\" ({round(time.time() - start_time_bt, 2)} s)\")\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()       # dropout is here, so call it\n",
    "                    yhat = self.model(x_val)\n",
    "                    val_loss = self.loss_fn(yhat, y_val).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                    \n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "            \n",
    "            # error_rate\n",
    "            if(test_loader != None):\n",
    "                preds, reals = self.evaluate(test_loader)\n",
    "\n",
    "                correct_pred, total_pred = 0, 0\n",
    "                for pred_i, real_i in zip(preds, reals):\n",
    "                    for pred_i_j, real_i_j in zip(pred_i, real_i):\n",
    "                        pred_i_j_one = np.argmax(pred_i_j)\n",
    "                        if(pred_i_j_one == real_i_j):\n",
    "                            correct_pred += 1\n",
    "                        total_pred += 1\n",
    "                error_rate = round(100 - correct_pred/total_pred*100, 2)\n",
    "                self.err_rates.append(error_rate/100)\n",
    "                \n",
    "            print(f\"({epoch}/{n_epochs}) Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\\t error rate: {error_rate if test_loader != None else None} %\", end =\"\")\n",
    "            print(f\" ({round(time.time() - start_time_ep, 2)} s)\")\n",
    "        \n",
    "        print(f\"Train time: ({round(time.time() - start_time_gl, 2)} s)\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader) -> \"(predicted, values)\":\n",
    "        \"\"\"The method evaluate performs the model evaluation\n",
    "\n",
    "        The method takes DataLoaders for the test dataset, batch size for mini-batch testing,\n",
    "        and number of features as inputs. Similar to the model validation, it iteratively\n",
    "        predicts the target values and calculates losses. Then, it returns two lists that\n",
    "        hold the predictions and the actual values.\n",
    "\n",
    "        Note:\n",
    "            This method assumes that the prediction from the previous step is available at\n",
    "            the time of the prediction, and only does one-step prediction into the future.\n",
    "\n",
    "        Args:\n",
    "            test_loader (torch.utils.data.DataLoader): DataLoader that stores test data\n",
    "\n",
    "        Returns:\n",
    "            list[float]: The values predicted by the model\n",
    "            list[float]: The actual values in the test set.\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                self.model.eval()         # dropout is here, so call it\n",
    "                yhat = self.model(x_test)\n",
    "                predictions.append(yhat.to(\"cpu\").detach().numpy())\n",
    "                values.append(y_test.to(\"cpu\").detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "\n",
    "    def check_adequacy(self, test_loader) -> None:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        correct_pred = {classname: 0 for classname in doggyDS.get_classes_list()}\n",
    "        total_pred = {classname: 0 for classname in doggyDS.get_classes_list()}\n",
    "\n",
    "        preds, reals = self.evaluate(test_loader)\n",
    "\n",
    "        for pred_i, real_i in zip(preds, reals):\n",
    "            for pred_i_j, real_i_j in zip(pred_i, real_i):\n",
    "                pred_i_j_one = np.argmax(pred_i_j)\n",
    "                if(pred_i_j_one == real_i_j):\n",
    "                    correct_pred[doggyDS.convert_tensor_to_labels([real_i_j])[0]] += 1\n",
    "                total_pred[doggyDS.convert_tensor_to_labels([real_i_j])[0]] += 1\n",
    "\n",
    "        accuracy_one_u = 0\n",
    "        accuracy_one_d = 0\n",
    "        for classname, correct_count in correct_pred.items():\n",
    "            accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "            accuracy_one_u += correct_count\n",
    "            accuracy_one_d += total_pred[classname]\n",
    "            print(f'Accuracy for class: {classname:7s} is {accuracy:.1f} %')\n",
    "        print(f'Accuracy for total: {100.0*accuracy_one_u/accuracy_one_d:.1f} %')\n",
    "        print(f\"Test time: ({round(time.time() - start_time, 2)} s)\")\n",
    "    \n",
    "    def find_loss(model, valid_loader, lr_min, lr_max, steps=50) -> None:\n",
    "        weight_decay = 0\n",
    "        #dataiter = iter(testloader)\n",
    "        #xs, ys = next(dataiter)\n",
    "        #xs, ys = xs.to(device), ys.to(device)\n",
    "        \n",
    "        i, N = 0, steps\n",
    "        X, Y = [], []\n",
    "        print(f\"{N}: \", end=\"\")\n",
    "        start_time = time.time()\n",
    "        for i in range(N+1):\n",
    "            cur_lr = lr_min + (lr_max-lr_min)*(i/N)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=cur_lr, weight_decay=weight_decay)\n",
    "            opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "            #loss = opt.train_step(xs, ys)\n",
    "            loss = 0\n",
    "            for x_batch, y_batch in valid_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss_l = opt.train_step(x_batch, y_batch)\n",
    "                loss += loss_l\n",
    "            print(f\"_{i+1}\", end=\"\")\n",
    "            \n",
    "            X.append(cur_lr)\n",
    "            Y.append(loss)\n",
    "        print(f\" ({round(time.time() - start_time, 2)} s)\")\n",
    "        plt.plot(X, Y)\n",
    "        #print(f\"X={X}, Y={Y}\")\n",
    "        df = pd.DataFrame(data={\"lr\": X, \"loss\": Y})\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "            print(df)\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"The method plots the calculated loss values for training and validation\n",
    "        \"\"\"\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        if(len(self.err_rates) > 0):\n",
    "            plt.plot(self.err_rates, label=\"Error rate\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "E8wZ2ozQQUPH"
   },
   "outputs": [],
   "source": [
    "class SrachNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.em1  = nn.Embedding(num_embeddings, 16).to(device)  \n",
    "        #self.lstmcell2  = nn.LSTMCell(16, 32).to(device)\n",
    "        self.lstm2  = nn.LSTM(16, 32).to(device)\n",
    "        \n",
    "        self.do3 = torch.nn.Dropout(0.3)\n",
    "        \n",
    "        self.dance4 = nn.Linear(32, 5).to(device)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.em1(sentence)\n",
    "        #x = self.lstmcell2(x)\n",
    "        lstm_out, _ = self.lstm2(embeds.view(len(sentence), 1, -1))\n",
    "        #x = F.relu(x)\n",
    "        #lstm_out = self.do3(lstm_out)\n",
    "        x = self.dance4(lstm_out.view(len(sentence), -1))\n",
    "        return x\n",
    "\n",
    "    def compute_l1_loss(self, w): # Так или Адомом. Правда у него только L2...\n",
    "        return torch.abs(w).sum()\n",
    "    def compute_l2_loss(self, w): # Так или Адомом\n",
    "        return torch.square(w).sum()\n",
    "#net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(SrachNet1(), (W, T, F))\n",
    "torch.save(SrachNet1().state_dict(), \"kokoko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsloaders_init(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 16, got 400",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 12\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(yhat, y_batch)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[72], line 15\u001b[0m, in \u001b[0;36mSrachNet1.forward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     13\u001b[0m embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem1(sentence)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#x = self.lstmcell2(x)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#x = F.relu(x)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#lstm_out = self.do3(lstm_out)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdance4(lstm_out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(sentence), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:772\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 772\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    774\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    775\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:697\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    693\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    694\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    695\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    696\u001b[0m                        ):\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    699\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    701\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:210\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    208\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    212\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 16, got 400"
     ]
    }
   ],
   "source": [
    "model = SrachNet1()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "train_loader = trainloader\n",
    "\n",
    "for x_batch, y_batch in train_loader:\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    yhat = model(x_batch)\n",
    "    loss = loss_fn(yhat, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "#     for x, y in zip(x_batch, y_batch):\n",
    "#         model.train()\n",
    "\n",
    "#         yhat = model(x)\n",
    "\n",
    "#         loss = loss_fn(yhat, y)\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1/3) 5879: "
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "LSTMCell: Expected input to be 1-D or 2-D but received 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m      9\u001b[0m opt \u001b[38;5;241m=\u001b[39m Optimization(model\u001b[38;5;241m=\u001b[39mmodel, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m opt\u001b[38;5;241m.\u001b[39mplot_losses()\n\u001b[1;32m     13\u001b[0m opt\u001b[38;5;241m.\u001b[39mcheck_adequacy(testloader)\n",
      "Cell \u001b[0;32mIn[46], line 94\u001b[0m, in \u001b[0;36mOptimization.train\u001b[0;34m(self, train_loader, val_loader, n_epochs, test_loader)\u001b[0m\n\u001b[1;32m     92\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     93\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 94\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m batch_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     96\u001b[0m gi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[46], line 49\u001b[0m, in \u001b[0;36mOptimization.train_step\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# dropout is here, so call it\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Makes predictions\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Computes loss\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(yhat, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m, in \u001b[0;36mSrachNet1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mem1(x)\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstmcell2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1182\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, hx: Optional[Tuple[Tensor, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), \\\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTMCell: Expected input to be 1-D or 2-D but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1184\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n",
      "\u001b[0;31mAssertionError\u001b[0m: LSTMCell: Expected input to be 1-D or 2-D but received 3-D tensor"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "learning_rate = 0.1\n",
    "weight_decay = 0.00001 # for L2\n",
    "\n",
    "model = SrachNet1()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "opt.train(trainloader, validloader, n_epochs)\n",
    "opt.plot_losses()\n",
    "\n",
    "opt.check_adequacy(testloader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
